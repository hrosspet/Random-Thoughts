AI versus us
=========
Recently it has been much discussed about the possible threats posed by artificial intelligence to humanity. I almost got persuaded by the arguments, that once an artificial agent with intelligence comparable to human is created, our existence is doomed for good, because once it reaches this level, it will self-improve with exponential speed and easily go beyond our imagination. Basically, what I understood from articles of various reknown people, we have 2 options, when this happens: either the AI will be malicious and kill us as an enemy or competitor, or it will extinct us only as a byproduct, because it will need some resources which are unfortunatelly essentially important for us.

With this reasoning, how can anyone even want to build an AI? This question has been stuck in my head for some time now, because exactly this is what we are trying to do in the project I have been working on the past 8 months.

Probably I will pose here more questions than answers, but here is my thought chain.

Imagine a human walking in a forest, when a wolf appears. The wolf notices the human and makes a long hungry howl. The hunt for the prey begins. Who will win this game for life and death?

One on one, who knows? The physical strength is certainly on wolf's side, will the human intelligence surprass it? If the human has tools, a knife, a spear, or even a gun, the winner is clear. But what if the wolf is not alone? What if there are 5 wolves? Will his gun shoot fast enough?

What if there are 10 wolves?

What if there are 10 of them and the human knows, that these 10 wolves are the only ones living?

By killing them he knows he would extinguish the whole species. Would he do it? With such a knowledge would he go to the forest in the first place? Wouldn't it be more clever to avoid the conflict? If he really needed to go there, he could use his intelligence to sneak in there unnoticed instead of harming the punny animals.

I think there is a big difference between having power and using it.

Or at least, there should be - if we had the possibility of harming no one and still continued in doing so, how could we expect anybody (AI, extra-terrestrials) acting differently to us?

If we can fix the human-animal relationship in such a general (and working) way, that we can imagine some super-human AI having this relationship with us, then we don't have to worry.

This is just one extreme case, which is convenient for my argument, because it is easy to contrast human and animal intelligence. However, I think there is a lot of work to be done on relationships even within our own species. Take for example how we treat underpriviledged, elderly or handicapped people. If we were able to fix our approach to these weaker groups (weaker in terms of intelligence, power, abilities, ...), if we were able to find a system - a moral code - which would make the co-existence of these groups mutually beneficial, or at least not harmful to any party, then we would certainly have nothing to worry about regarding potential super-human AI.
